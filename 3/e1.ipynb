{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os,shutil,glob,re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = './brown'\n",
    "os.chdir(rootdir)\n",
    "outfilename = \"brown.txt\"\n",
    "prevdir = '../'\n",
    "with open(prevdir+outfilename, 'wb') as outfile:\n",
    "    for filename in glob.glob('*'):\n",
    "        if filename == outfilename:\n",
    "            continue\n",
    "        with open(filename, 'rb') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "os.chdir(prevdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(file_name):\n",
    "    file = open(file_name,\"r\",encoding='utf-8')\n",
    "    text= file.read()\n",
    "    file.close()\n",
    "    words_list = text.split()\n",
    "    removal_dict = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    tokenized_words = [w.split('/')[0] for w in words_list]\n",
    "    clean_words_list = [word.lower() for word in tokenized_words]\n",
    "    print(clean_words_list[:100])\n",
    "    return clean_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(line):\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    return line.translate(table)\n",
    "\n",
    "def ENwordValid(word):\n",
    "    if(word.isalpha() and (word[0].islower() or word.istitle())):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def numberSpecialFree(word):\n",
    "    if word == \"â€“\":\n",
    "        return False\n",
    "    return not any(word.isdigit() for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(file_name, brown = False, dir = os.getcwd(),dictionary = {}, text_list = []):\n",
    "            with open(dir + \"/\" + file_name,'r',encoding='utf-8') as file:     \n",
    "                for line in file:\n",
    "                    for word in line.split():\n",
    "                        word = removePunctuation(word.split(\"/\")[0])\n",
    "                        if(ENwordValid(word)):\n",
    "                            word = word.lower()\n",
    "                            text_list.append(word)\n",
    "                            if(word in dictionary):\n",
    "                                dictionary[word] = dictionary[word] + 1\n",
    "                            else: dictionary[word] = 1\n",
    "            d2 = dictionary.copy()\n",
    "            l2 = text_list.copy()\n",
    "            dictionary.clear()\n",
    "            text_list.clear()\n",
    "            return d2,l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2,clean_words = preprocess_file('brown.txt') #data_cleaning('brown.txt')\n",
    "counts = dict(Counter(clean_words))\n",
    "clean_words_set = set(clean_words) \n",
    "unique_clean_words = (list(clean_words_set))\n",
    "counter_pairs = Counter(zip(clean_words, clean_words[1:]))\n",
    "counter_pairs = dict(counter_pairs)\n",
    "freq_the = counts['the']\n",
    "freq_in = counts['in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_the_freq = {}\n",
    "dict_in_freq = {}\n",
    "for k,v in counter_pairs.items():\n",
    "    if(k[0]=='the'):\n",
    "        dict_the_freq[k[1]]=v\n",
    "    if(k[0]=='in'):\n",
    "        dict_in_freq[k[1]]=v\n",
    "absent_word_dict_the_freq = np.setdiff1d(unique_clean_words,list(dict_the_freq.keys()))\n",
    "absent_word_dict_in_freq = np.setdiff1d(unique_clean_words,list(dict_in_freq.keys()))\n",
    "dict_the_freq.update(dict((el,0) for el in absent_word_dict_the_freq))\n",
    "dict_in_freq.update(dict((el,0) for el in absent_word_dict_in_freq))\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_the_prob = {}\n",
    "dict_in_prob = {}\n",
    "dict_the_prob = {k: v / freq_the for k, v in dict_the_freq.items()}\n",
    "dict_in_prob = {k: v / freq_in for k, v in dict_in_freq.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "top_20_the = dict(sorted(dict_the_freq.items(), key = itemgetter(1), reverse = True)[:20])\n",
    "top_20_in = dict(sorted(dict_in_freq.items(), key = itemgetter(1), reverse = True)[:20])\n",
    "\n",
    "plt.plot(list(top_20_the.keys()), list(top_20_the.values()),linewidth=4)\n",
    "plt.title(\"Frequency Distribution of THE\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(top_20_in.keys()), list(top_20_in.values()),linewidth=4)\n",
    "plt.title(\"Frequency Distribution of IN\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 20 words in \\\"THE\\\" distribution along with their frequency count\\n\\n\")\n",
    "print(top_20_the)\n",
    "print(\"\\n\\nTop 20 words in \\\"IN\\\" distribution along with their frequency count\\n\\n\")\n",
    "print(top_20_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "expected_value_the = 0 \n",
    "expected_value_in = 0 \n",
    "for k,v in dict_the_prob.items():\n",
    "    if(v==0):\n",
    "        continue\n",
    "    expected_value_the = expected_value_the + v * math.log(v,2)\n",
    "for k,v in dict_in_prob.items():\n",
    "    if(v==0):\n",
    "        continue\n",
    "    expected_value_in = expected_value_in + v * math.log(v,2)\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"\\n The expected value of \\\"The\\\" distrivbution = \" +  str(-1 * expected_value_the) + \"  \\n\")\n",
    "print(\"\\n The expected value of \\\"In\\\" distrivbution = \" +  str(-1 * expected_value_in) + \"  \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
